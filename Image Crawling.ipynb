{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "33040239",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "검색어 입력 : 송중기\n",
      "크롤링할 갯수 입력(최대 50개) : 30\n",
      "https://search.naver.com/search.naver?where=image&sm=tab_jum&query=%EC%86%A1%EC%A4%91%EA%B8%B0\n",
      "1\n",
      "1\n",
      "Imgae Crawling is done.\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup as bs\n",
    "from urllib.request import urlopen\n",
    "from urllib.parse import quote_plus\n",
    "import requests\n",
    "\n",
    "baseUrl = 'https://search.naver.com/search.naver?where=image&sm=tab_jum&query='  #네이버 검색창\n",
    "plusUrl = input('검색어 입력 : ')\n",
    "crawl_num = int(input('크롤링할 갯수 입력(최대 50개) : '))\n",
    "\n",
    "url = baseUrl + quote_plus(plusUrl) # urllib.parse 로 한글도 인식할 수 있게끔\n",
    "print(url)\n",
    "html = urlopen(url)\n",
    "\n",
    "\n",
    "#html = urlopen(url)\n",
    "soup = bs(html, \"html.parser\") \n",
    "img = soup.find_all('img')    # img로 된거 찾고\n",
    "\n",
    "n = 1\n",
    "print(len(img))\n",
    "for i in img:\n",
    "    print(n)\n",
    "    imgUrl = i['src'].split('&')[0]   # &로 나눈 이유는 \n",
    "    #네이버에서 &값 뒤에 있는 값으로 싸이즈를 조절하는 것 같아서 원본 싸이즈를 가져오기 위해 잘랐다.\n",
    "    with urlopen(imgUrl) as f:\n",
    "        with open('./img/img00 ('+str(n)+').jpg','wb') as h :  # py파일이 있는 디렉토리 안에 img 파일을 찾아서 img 1,2,3 으로 저장\n",
    "            img = f.read()\n",
    "            h.write(img)\n",
    "    n+=1\n",
    "    if n> crawl_num:\n",
    "        break\n",
    "\n",
    "print(\"Imgae Crawling is done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fbe62c60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['images (5).jpg',\n",
       " 'images (3).jpg',\n",
       " '다운로드 (2).jpg',\n",
       " '다운로드 (1).jpg',\n",
       " 'images (4).jpg',\n",
       " 'images (1).jpg',\n",
       " 'images (6).jpg',\n",
       " 'ㄴ.jpg',\n",
       " 'images (2).jpg',\n",
       " 'images.jpg',\n",
       " '다운로드.jpg',\n",
       " 'images (7).jpg']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# 주어진 디렉토리에 있는 항목들의 이름을 담고 있는 리스트를 반환합니다.\n",
    "# 리스트는 임의의 순서대로 나열됩니다.\n",
    "file_path = \"/home/qkrwnstj/다운로드/birme-512x512\" \n",
    "#\"/home/qkrwnstj/anaconda3/envs/pytorch/img\"\n",
    "file_names = os.listdir(file_path)\n",
    "file_names\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d440c553",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 1\n",
    "for name in file_names:\n",
    "    src = os.path.join(file_path, name)\n",
    "    dst = \"leeex (\"+str(i) + ').jpg'\n",
    "    #\"kimex (\"+str(i) + ').jpg'\n",
    "    dst = os.path.join(file_path, dst)\n",
    "    os.rename(src, dst)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "fcd1de00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "박보검 검색\n",
      "임시완 검색\n",
      "강동원 검색\n",
      "이종석 검색\n",
      "조세호 검색\n",
      "안재홍 검색\n",
      "다운로드 완료!\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from urllib.parse import quote_plus\n",
    "from pathlib import Path\n",
    "\n",
    "baseUrl = 'https://search.naver.com/search.naver?where=image&sm=tab_jum&query='\n",
    "\n",
    "animal_list = ['dog', 'cat', 'bear']\n",
    "keyword_list = [['박보검', '임시완'], ['강동원', '이종석'], ['조세호', '안재홍']]\n",
    "\n",
    "idx = 0\n",
    "\n",
    "for arr in keyword_list:\n",
    "    Path('./img/' + animal_list[idx]).mkdir(parents=True, exist_ok=True)\n",
    "    for keyword in arr:\n",
    "        Path('./img/' + animal_list[idx] + '/' + keyword).mkdir(parents=True, exist_ok=True)\n",
    "        print(keyword + ' 검색')\n",
    "        url = baseUrl + quote_plus(keyword)\n",
    "        html = urlopen(url)\n",
    "        soup = bs(html, \"html.parser\")\n",
    "        img = soup.find_all('img')\n",
    "        n = 1\n",
    "        for i in img:\n",
    "            \n",
    "            imgUrl = i['src'].split('&')[0]\n",
    "            with urlopen(imgUrl) as f:\n",
    "                with open('./img/' + animal_list[idx] + '/' + keyword + '/' + keyword + str(n)+'.jpg', 'wb') as h:\n",
    "                    img = f.read()\n",
    "                    h.write(img)\n",
    "            n += 1\n",
    "    idx += 1\n",
    "print('다운로드 완료!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
